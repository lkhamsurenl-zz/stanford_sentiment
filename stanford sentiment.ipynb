{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment classification \n",
    "\n",
    "Using https://ai.stanford.edu/~amaas/data/sentiment/ we perform simple sentiment classification. \n",
    "\n",
    "The notebook is supposed to demonstrate quick ML development process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data can be found in https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "import urllib.request\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    urllib.request.urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', file_name)\n",
    "\n",
    "# unzip if not exists\n",
    "if not os.path.exists('aclImdb'):\n",
    "    !tar -xvzf {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "In our case, the problem is simple binary classification problem with positive or negative review and our dataset\n",
    "is balanced, so for MVP we propose to use accuracy as main metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of pos files available: 12500\n"
     ]
    }
   ],
   "source": [
    "full_train_dir = 'aclImdb/train'\n",
    "full_train_pos_dir = f'{full_train_dir}/pos'\n",
    "full_train_neg_dir = f'{full_train_dir}/neg'\n",
    "full_train_pos_filenames = [f for f in listdir(full_train_pos_dir) if isfile(join(full_train_pos_dir, f))]\n",
    "full_train_neg_filenames = [f for f in listdir(full_train_neg_dir) if isfile(join(full_train_neg_dir, f))]\n",
    "\n",
    "print(f'total number of pos files available: {len(full_train_pos_filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_pos_filenames=10000\n",
      "train_neg_filenames=10000\n",
      "valid_pos_filenames=2500\n",
      "valid_neg_filenames=2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_num_files = 10000\n",
    "train_pos_filenames = full_train_pos_filenames[:train_num_files]\n",
    "train_neg_filenames = full_train_neg_filenames[:train_num_files]\n",
    "\n",
    "valid_pos_filenames = full_train_pos_filenames[train_num_files:]\n",
    "valid_neg_filenames = full_train_neg_filenames[train_num_files:]\n",
    "\n",
    "print(f\"\"\"\n",
    "train_pos_filenames={len(train_pos_filenames)}\n",
    "train_neg_filenames={len(train_neg_filenames)}\n",
    "valid_pos_filenames={len(valid_pos_filenames)}\n",
    "valid_neg_filenames={len(valid_neg_filenames)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_content(pos_filenames, neg_filenames):\n",
    "    texts = []\n",
    "    labels = [1.0 for _ in range(len(pos_filenames))] + [0.0 for _ in range(len(neg_filenames))]\n",
    "    for name in pos_filenames:\n",
    "        with open(f'{full_train_dir}/pos/{name}', 'r') as f:\n",
    "            texts.append(f.read())\n",
    "            \n",
    "    for name in neg_filenames:\n",
    "        with open(f'{full_train_dir}/neg/{name}', 'r') as f:\n",
    "            texts.append(f.read())\n",
    "            \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts, train_y = get_content(train_pos_filenames, train_neg_filenames)\n",
    "valid_texts, valid_y = get_content(valid_pos_filenames, valid_neg_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts=20000; train_y=20000\n"
     ]
    }
   ],
   "source": [
    "print(f'train_texts={len(train_texts)}; train_y={len(train_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_X = vectorizer.transform(valid_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'solver': ['lbfgs', 'liblinear', 'sag'],\n",
    "                     'penalty': ['l1', 'l2']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lkhamsurenl/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/lkhamsurenl/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "nan (+/-nan) for {'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.868 (+/-0.014) for {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'penalty': 'l1', 'solver': 'sag'}\n",
      "0.885 (+/-0.013) for {'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.885 (+/-0.013) for {'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.885 (+/-0.013) for {'penalty': 'l2', 'solver': 'sag'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89      2500\n",
      "         1.0       0.88      0.90      0.89      2500\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lkhamsurenl/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/lkhamsurenl/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "nan (+/-nan) for {'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.868 (+/-0.014) for {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'penalty': 'l1', 'solver': 'sag'}\n",
      "0.884 (+/-0.013) for {'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.884 (+/-0.013) for {'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.884 (+/-0.013) for {'penalty': 'l2', 'solver': 'sag'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89      2500\n",
      "         1.0       0.88      0.90      0.89      2500\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        LogisticRegression(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X, train_y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    valid_y, valid_preds = valid_y, clf.predict(valid_X)\n",
    "    print(classification_report(valid_y, valid_preds))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.93      0.93     10000\n",
      "         1.0       0.93      0.94      0.93     10000\n",
      "\n",
      "    accuracy                           0.93     20000\n",
      "   macro avg       0.93      0.93      0.93     20000\n",
      "weighted avg       0.93      0.93      0.93     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How is the performance of the best model on training?\n",
    "train_y, train_preds = train_y, clf.predict(X)\n",
    "print(classification_report(train_y, train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see model is definitely overfitting into the current training set. There are couple things to note:\n",
    "1. Even with simple logistic regression, we are already overfitting into the model. More sophisticated model with current dataset is likely to overfit even further (MLP, tree based models)\n",
    "2. Having more data likely to help us generalize better\n",
    "3. We should try out better regularization (we have used L2 above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
